model:
  provider: ollama
  model: llama3
  base_url: http://127.0.0.1:11434
  keep_alive: "5m"

inference:
  temperature: 0.1
  top_p: 0.9
  top_k: 40
  num_ctx: 8192
  num_gpu: 1
  num_thread: 8
  num_predict: 512

logging:
  log_level: "INFO"
  log_file: "logs/llm.log"
