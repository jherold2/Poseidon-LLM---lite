"""Skeleton DPO training entrypoint with MLflow integration."""

from __future__ import annotations

import argparse
import logging
from pathlib import Path
from typing import Mapping

import yaml

from tuning.eval import runner as eval_runner


logger = logging.getLogger(__name__)


def load_recipe(path: Path | str) -> Mapping:
    recipe_path = Path(path)
    payload = yaml.safe_load(recipe_path.read_text())
    if payload.get("type") not in {"dpo", None}:
        raise ValueError(f"Recipe {recipe_path} is not a DPO recipe")
    payload.setdefault("type", "dpo")
    return payload


def start_mlflow_run(recipe: Mapping, disable_mlflow: bool = False):
    try:
        import mlflow
    except ImportError:  # pragma: no cover - optional dependency
        mlflow = None

    if disable_mlflow or mlflow is None:
        class _Noop:
            def __enter__(self):
                return None

            def __exit__(self, exc_type, exc, tb):
                return False

        return _Noop()

    mlflow.set_experiment(recipe.get("logging", {}).get("mlflow_experiment", "poseidon-tuning"))
    return mlflow.start_run(run_name=f"dpo::{recipe['name']}")


def run_training(recipe: Mapping) -> Mapping[str, float]:
    """Placeholder DPO training loop.

    Integrate this function with TRL's `DPOTrainer` once preference data and
    reward models are wired in.
    """

    logger.info("Pretending to run DPO with base %s", recipe.get("base_model"))
    logger.debug("Hyperparameters: %s", recipe.get("hyperparameters", {}))
    return {"preference_loss_final": 0.0}


def main(argv: list[str] | None = None) -> None:
    parser = argparse.ArgumentParser(description="Launch a DPO fine-tuning run")
    parser.add_argument("recipe", type=Path, help="Path to DPO recipe YAML")
    parser.add_argument(
        "--eval-spec",
        type=Path,
        default=Path("tuning/eval/specs/safety_redteam_v1.yaml"),
        help="Eval spec to run after training",
    )
    parser.add_argument(
        "--predictions",
        type=Path,
        help="Predictions JSONL generated by the tuned model",
    )
    parser.add_argument("--no-mlflow", action="store_true", help="Disable MLflow logging")
    args = parser.parse_args(argv)

    recipe = load_recipe(args.recipe)
    logging.basicConfig(level=logging.INFO)

    with start_mlflow_run(recipe, disable_mlflow=args.no_mlflow):
        try:
            import mlflow
        except ImportError:  # pragma: no cover
            mlflow = None

        if mlflow and not args.no_mlflow:
            mlflow.log_dict(recipe, "recipes/dpo_recipe.yaml")
            mlflow.set_tag("stage", "rlhf")

        metrics = run_training(recipe)
        if mlflow and not args.no_mlflow and metrics:
            mlflow.log_metrics(metrics)

        if args.eval_spec and args.predictions:
            eval_runner.run_eval(
                spec_path=args.eval_spec,
                predictions_path=args.predictions,
                enable_mlflow=not args.no_mlflow,
            )
        elif args.eval_spec and not args.predictions:
            logger.warning("Skipping eval spec %s because --predictions was not provided", args.eval_spec)


if __name__ == "__main__":
    main()
